input {
  file {
    path => "/usr/share/logstash/logs/access.log"  # <-- your path
    start_position => "beginning"
    sincedb_path => "/dev/null"
    # Your lines are valid JSON at top-level ECS fields; decode to root
    codec => json
  }
}

filter {
  # Turn the joined JSON into fields
  json {
    source => "message"
    remove_field => ["message"]
    tag_on_failure => ["_jsonparsefailure"]
  }

  sleep {
    time => 0.01        # 0.01 seconds = 10 ms
    every => 1          # sleep every event (can change to N to sleep every Nth event)
  }

  # Parse the log_timestamp field to @timestamp
  date {
    match => ["log_timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # The JSON value contains literal quotes: "\"POST /... HTTP/1.1\""
  # Strip the wrapping quotes first
  mutate {
    gsub => ["request", '^"(.*)"$', '\1']
  }

  # Parse request into method, path, version (ECS-ish fields)
  grok {
    match => {
      "request" => '^%{WORD:[http][request][method]} %{URIPATHPARAM:[url][path]} HTTP/%{NUMBER:[http][version]}$'
    }
    tag_on_failure => ["_request_grok_fail"]
  }

  # Normalize a few fields toward ECS
  mutate {
    rename => {
      "ip"     => "[client][ip]"
      "userid" => "[user][name]"
      "status" => "[http][response][status_code]"
      "size"   => "[http][response][body][bytes]"
    }
    convert => {
      "[http][response][status_code]" => "integer"
      "[http][response][body][bytes]" => "integer"
      "line" => "integer"
    }
    add_field => { "event.dataset" => "custom.web" }
  }
}

output {
  elasticsearch {

    ## static name
    # other examples
    ## dinamic index name
    # index => "${ELASTICSEARCH_INDEX_NAME}-%{+dd-MM-YYYY-HH-mm}"
    # index => "${ELASTICSEARCH_INDEX_NAME}-%{userid}-%{+dd-MM-YYYY-HH-mm}"
    
    hosts => ["${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}"]
    index => "${ELASTICSEARCH_INDEX_NAME}"
  }

  stdout { codec => rubydebug }
}